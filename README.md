# RAG Comparison Tool: Hybrid Search vs Naive RAG

A comprehensive evaluation framework for comparing Retrieval-Augmented Generation (RAG) approaches using the RAGAS evaluation methodology. This tool implements and benchmarks **Hybrid Search with Reranking** against **Naive RAG** to provide objective performance metrics.

![RAG Comparison Tool](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Flask](https://img.shields.io/badge/Flask-2.3+-red.svg)
![RAGAS](https://img.shields.io/badge/RAGAS-0.1+-purple.svg)

## ğŸš€ Features

### **Hybrid Search Implementation**
- **BM25 + Dense Retrieval**: Combines lexical and semantic search approaches
- **Cohere Reranking**: Advanced reranking using `rerank-english-v3.0` model
- **Ensemble Weighting**: Configurable balance between sparse and dense retrieval
- **Contextual Compression**: Intelligent document filtering and ranking

### **Comprehensive Evaluation**
- **RAGAS Framework**: 6 core evaluation metrics
  - Answer Relevancy
  - Faithfulness  
  - Context Precision
  - Context Recall
  - Answer Similarity
  - Answer Correctness
- **Fallback Evaluation**: Basic similarity metrics when RAGAS unavailable
- **Token Tracking**: Monitor API usage and costs
- **Success Rate Monitoring**: Track processing efficiency

### **User Experience**
- **Web Interface**: Intuitive drag-and-drop file upload
- **Real-time Progress**: Live evaluation updates
- **Interactive Results**: Side-by-side metric comparison
- **Error Handling**: Graceful degradation with helpful messages

## ğŸ“Š Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PDF Upload    â”‚    â”‚  Ground Truth    â”‚    â”‚   Web Interface â”‚
â”‚                 â”‚    â”‚   JSON File      â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                     â”‚                                   â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
          â”‚  Document Processing â”‚                       â”‚
          â”‚  - Text Extraction   â”‚                       â”‚
          â”‚  - Chunking          â”‚                       â”‚
          â”‚  - Embedding         â”‚                       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                     â”‚                                   â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚     Naive RAG        â”‚    â”‚  Hybrid RAG +   â”‚ â”‚
          â”‚  - FAISS Vector DB   â”‚    â”‚   Reranking     â”‚ â”‚
          â”‚  - Simple Retrieval  â”‚    â”‚  - BM25 + Dense â”‚ â”‚
          â”‚                      â”‚    â”‚  - Cohere       â”‚ â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                     â”‚                          â”‚         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                               â”‚                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                    â”‚   RAGAS Evaluation   â”‚              â”‚
                    â”‚  - Multiple Metrics  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  - Comparison Report â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+
- OpenAI API Key
- Cohere API Key (optional, for reranking)

### Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/rag-comparison-tool.git
cd rag-comparison-tool
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys
```

4. **Run the application**
```bash
python app.py
```

5. **Open your browser**
```
http://localhost:5000
```

## ğŸ“ Usage

### 1. Prepare Your Data

**PDF Document**: Any PDF containing the text content for your knowledge base.

**Ground Truth JSON**: Q&A pairs in the following format:
```json
[
    {
        "question": "What is RAG in Natural Language Processing?",
        "answer": "RAG (Retrieval-Augmented Generation) combines information retrieval with text generation..."
    }
]
```

### 2. Run Evaluation

1. Upload your PDF document and ground truth JSON file
2. Click "Start RAG Comparison"
3. Monitor real-time progress
4. Review detailed comparison results

### 3. Interpret Results

The tool provides:
- **Metric Scores**: Individual performance scores for each approach
- **Improvement Analysis**: Percentage improvements/declines per metric
- **Token Usage**: API cost tracking
- **Success Rate**: Processing efficiency metrics

## ğŸ“Š Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **Answer Relevancy** | How relevant the generated answer is to the question |
| **Faithfulness** | How faithful the answer is to the retrieved context |
| **Context Precision** | Precision of the retrieved context |
| **Context Recall** | Recall of the retrieved context |
| **Answer Similarity** | Similarity between generated and ground truth answers |
| **Answer Correctness** | Overall correctness of the generated answer |

## ğŸ”§ Configuration

### Environment Variables
```bash
# Required
OPENAI_API_KEY=sk-...                    # OpenAI API key
COHERE_API_KEY=...                       # Cohere API key (optional)

# Optional
FLASK_DEBUG=True                         # Enable debug mode
FLASK_PORT=5000                          # Application port
```

### Model Configuration
Modify the `Config` class in `app.py`:
```python
class Config:
    EMBEDDING_MODEL = 'BAAI/bge-base-en-v1.5'    # Embedding model
    LLM_MODEL = "gpt-4"                           # Language model
    CHUNK_SIZE = 512                              # Document chunk size
    RETRIEVAL_K = 5                               # Documents to retrieve
    ENSEMBLE_WEIGHTS = [0.6, 0.4]                 # Dense vs sparse weights
```

## ğŸ—ï¸ Project Structure

```
rag-comparison-tool/
â”œâ”€â”€ app.py                      # Main Flask application
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ sample_ground_truth.json   # Example ground truth format
â”œâ”€â”€ logs/                      # Application logs
â”œâ”€â”€ uploads/                   # Temporary file storage
â””â”€â”€ README.md                  # This file
```

## ğŸ”¬ Example Results

```
ğŸ† Evaluation Results

Naive RAG:
â”œâ”€â”€ Answer Relevancy: 72.3%
â”œâ”€â”€ Faithfulness: 68.9%
â”œâ”€â”€ Context Precision: 71.2%
â””â”€â”€ Answer Similarity: 69.8%

Hybrid RAG + Reranking:
â”œâ”€â”€ Answer Relevancy: 84.7% (â†— 17.2%)
â”œâ”€â”€ Faithfulness: 79.3% (â†— 15.1%)
â”œâ”€â”€ Context Precision: 82.6% (â†— 16.0%)
â””â”€â”€ Answer Similarity: 78.4% (â†— 12.3%)

Overall Winner: Hybrid RAG with Reranking
Token Usage: 2,347 tokens across 15 queries
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup
```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black app.py
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **RAGAS Team** for the evaluation framework
- **LangChain** for the retrieval components
- **Cohere** for the reranking API
- **OpenAI** for the language models

## ğŸ“ˆ Citation

If you use this tool in your research, please cite:

```bibtex
@software{rag_comparison_tool,
  title={RAG Comparison Tool: Hybrid Search vs Naive RAG},
  author={Your Name},
  year={2025},
  url={https://github.com/yourusername/rag-comparison-tool}
}
```

## ğŸ†˜ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/rag-comparison-tool/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/rag-comparison-tool/discussions)
- **Documentation**: [Wiki](https://github.com/yourusername/rag-comparison-tool/wiki)

## ğŸ”® Roadmap

- [ ] Support for multiple document formats (DOCX, TXT, etc.)
- [ ] Advanced chunking strategies
- [ ] Custom reranking models
- [ ] Batch evaluation capabilities
- [ ] Export results to CSV/PDF
- [ ] Integration with more LLM providers
- [ ] Docker containerization
- [ ] Kubernetes deployment

---

â­ **Star this repository** if you find it useful!

[![GitHub stars](https://img.shields.io/github/stars/yourusername/rag-comparison-tool.svg?style=social&label=Star)](https://github.com/yourusername/rag-comparison-tool)
[![GitHub forks](https://img.shields.io/github/forks/yourusername/rag-comparison-tool.svg?style=social&label=Fork)](https://github.com/yourusername/rag-comparison-tool/fork)